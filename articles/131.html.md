---
title: 梶研 [ST-GCNやるよ]
category: kajilab/2024年度/06月
tags: date:2024-06-04
created_at: '2024-06-03T18:17:33+09:00'
updated_at: '2024-06-04T02:39:49+09:00'
published: true
number: 131
---

# ST-GCNやるよ

## 出席率
- 3年セミナー：??%

## スケジュール
### 短期的な予定
- [ ] mocopi と お料理センシング
    - [x] シーンとランドマークを決める(~2月上旬)
    - [x] SVM で動作判別する
    - [x] 機械学習を深める
    - [ ] お料理センシング
        - [x] お料理でどんな動作があるかを知る
        - [x] レシピを決める
        - [x] 関節を3次元座標に変換する
        - [ ] ~~関節を2次元座標に変換する~~
        - [x] ST-GCN を大体理解
        - [ ] ST-GCN で動作推定
        - [ ] 伊達巻きで動作推定する
        - [ ] レシピ(手順書)を元に動作推定を補う
    - [ ] 論文書く
    - [ ] 発表
- [ ] BookWorm
    - [x] Pasori と デスクトップアプリを接続する(技術検証)
    - [x] nfc読み込み機能 & 画面を作る
    - [ ] API と連携させる
    - [x] 管理者画面を作る

### 長期的な予定
- 6月 精度は置いておき、動作認識を完成させる
- 7月 リファクターと精度を向上させる
- 8月 仕上げ
- 9月 論文書き始め
- 10月末 論文提出
- 12月 WiNF当日

## 進捗報告
### 目的
料理中の動作を mocopi を使ってセンシングする。
このデータから最終的に位置推定を行う。
- 一定の区間でどの動作をしているかを当てる (クラス分類)
- 料理の手順を元にシーン検知を補正する
    - 例) 焼く動作 → 卵割る動作 はおかしい
- 位置とシーンを相補的に補正する
    - 例) 冷蔵庫の前で焼く動作 はおかしい

### 目標
12月頃の WiNF に出たい
(10月末 論文完成)


### 適当なデータ取った
お料理は何度もデータ取るのが大変なため、
データが取りやすい動作を1分ずつ行った
```
体を曲げる運動
両手を上げ下げ
体を捻る
ジャンプ
何もしない
```

<iframe width="1256" height="788" src="https://www.youtube.com/embed/L1PYoPBId8s" title="mocopiセンシング 適当な動作" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 扱いやすいように結合&csvに変換する
```py
bending_exercise_bvhp = BVHparser("./data/bending_exercise.bvh")
bending_exercise_motion_df = bending_exercise_bvhp.get_motion_df()
bending_exercise_motion_df["label"] = 1

hands_updown_bvhp = BVHparser("./data/hands_updown.bvh")
hands_updown_motion_df = hands_updown_bvhp.get_motion_df()
hands_updown_motion_df["label"] = 2

...

# 全ての motion_df を結合
motion_df = pd.concat(
    [
        bending_exercise_motion_df,
        hands_updown_motion_df,
        twist_your_body_motion_df,
        jump_motion_df,
        do_nothing_motion_df,
    ]
)

# csv に保存
motion_df.to_csv("csv/train.csv", index=False)
```

※ テスト用も同様

### 自作BVHパーサライブラリでエラー吐いた
アップデートの影響で形式が微妙に変わっていた
半角スペースで split して frame time を取り出していたので `Time:0.016667` となって数値化に失敗してた
```diff
骨格のデータ
...
- Frame Time: 0.016667
+ Frame Time:0.016667
...
モーションのデータ
```

`:` で判定するように直した

### 前にBVH用に改変した ST-GCN に入れてみる
脳内デバッグなので当然エラーが出まくった → いい感じに修正

現在のエラー
```
ValueError: not enough values to unpack (expected 4, got 2)
```
入力した tensor の次元が違うらしい(本来:4次元, 実際:2次元)
csv の形式なのでそうだろうけど...

元のプログラムがどんなデータを入力していたかを追ってみる

そういえば、各関節が相対座標なのを世界座標にするやつ使ってない...


## 進路関係
気が向いたので、ゆめみパスポートに応募してみた
ゆめみのコーディング試験を受けれるやつ. 腕試しとしてやってみる
https://hrmos.co/pages/yumemi/jobs/101000000010
↓
ゆめみパスポートげっとした
(YUMEMIを受ける場合、コーディングテストスキップできる)
お金もらえなかった(終了済み)


## 余談
### 焼肉たべた
pluszero組で食べた

<img width="3024" alt="IMG_6879.JPG (4.2 MB)" src="https://img.esa.io/uploads/production/attachments/21347/2024/06/04/148142/2671b052-009c-4e1e-8ee0-6f5ce3ff81f2.JPG">


### ローカル5G触ってます
中村先生からサークルへの依頼で、ローカル5Gを学生目線で伝えるプロジェクト（？）をやってます
with makino先輩
<img width="4032" alt="IMG_6873.JPG (3.4 MB)" src="https://img.esa.io/uploads/production/attachments/21347/2024/06/04/148142/839391ee-4d7c-46a0-ab50-69f88073f290.JPG">

オーキャンに出すので makino先輩 と僕が交互に梶研エリアから消える可能性があります

